{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca2be51c-9f93-4b16-847b-45a26246770d",
   "metadata": {},
   "source": [
    "# 1) Forest Classification Validation Analysis\n",
    "\n",
    "    This code performs a comprehensive validation analysis on forest classification predictions by comparing predicted classes against actual target classes in CSV files, calculating various metrics including F1-score, precision, recall, Cohen's Kappa, and AUC for each class, and generating confusion matrices for visual assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4e2d17-555e-4820-bef3-af0a8a22b65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix, cohen_kappa_score, accuracy_score, precision_score, recall_score, roc_auc_score, roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.exceptions import UndefinedMetricWarning  # Importuj to\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "csv_folder = '/path/to/extracted_feature_values/csv'\n",
    "output_file_path = '/path/to/save/tiles_accuracy.txt'\n",
    "confusion_matrix_folder = '/path/to/save/confusion_matrix/'\n",
    "\n",
    "csv_files = [file for file in os.listdir(csv_folder) if file.endswith('.csv')]\n",
    "\n",
    "start = time.process_time()\n",
    "\n",
    "confusion_matrices = []\n",
    "with open(output_file_path, 'w') as output_file:\n",
    "    for csv_file_name in csv_files:\n",
    "        csv_file_path = os.path.join(csv_folder, csv_file_name)\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "        df['target'] = pd.to_numeric(df['target'], errors='coerce')\n",
    "        df['predicted'] = pd.to_numeric(df['predicted'], errors='coerce')\n",
    "        \n",
    "        df_filtered = df[(df['target'].between(1, 6)) & (df['predicted'].between(1, 6))]\n",
    "        if df_filtered.empty:\n",
    "            print(f\"No valid data for {csv_file_name}, skipping.\")\n",
    "            continue\n",
    "        \n",
    "        y_true = df_filtered['target']\n",
    "        y_pred = df_filtered['predicted']\n",
    "\n",
    "        f1_scores, precisions, recalls = {}, {}, {}\n",
    "        \n",
    "        for class_label in range(1, 7):\n",
    "            if (class_label in y_true.values) and (class_label in y_pred.values):\n",
    "                precision, recall, f1, _ = precision_recall_fscore_support(y_true == class_label, y_pred == class_label, average='binary', zero_division=0)\n",
    "                f1_scores[class_label] = f\"{f1:.4f}\"\n",
    "                precisions[class_label] = f\"{precision:.4f}\"\n",
    "                recalls[class_label] = f\"{recall:.4f}\"\n",
    "            else:\n",
    "                f1_scores[class_label] = \"n/a\"\n",
    "                precisions[class_label] = \"n/a\"\n",
    "                recalls[class_label] = \"n/a\"    \n",
    "\n",
    "        overall_accuracy_all_classes = accuracy_score(df_filtered['target'], df_filtered['predicted'])\n",
    "        kappa_score_all_classes = cohen_kappa_score(df_filtered['target'], df_filtered['predicted'])\n",
    "\n",
    "        class_6_indices = df_filtered['target'] == 6\n",
    "        overall_accuracy_class_6 = accuracy_score(df_filtered[class_6_indices]['target'], df_filtered[class_6_indices]['predicted'])\n",
    "\n",
    "        classes = list(range(1, 7))\n",
    "        y_true_binarized = label_binarize(df_filtered['target'], classes=classes)\n",
    "        auc_per_class = {}\n",
    "        roc_per_class = {}\n",
    "        for class_label in classes:\n",
    "            binary_labels = df_filtered['target'] == class_label\n",
    "            if binary_labels.any():  \n",
    "\n",
    "                fpr, tpr, thresholds = roc_curve(binary_labels, df_filtered['predicted'] == class_label)\n",
    "                auc_value = auc(fpr, tpr)\n",
    "                auc_per_class[f'class {class_label}'] = \"{:.4f}\".format(auc_value)\n",
    "                \n",
    "                roc_per_class[class_label] = (fpr, tpr, thresholds)\n",
    "            else:\n",
    "                auc_per_class[f'class {class_label}'] = \"n/a\"\n",
    "                roc_per_class[class_label] = (None, None, None)\n",
    "        \n",
    "        user_accuracy_per_class = {}\n",
    "        producer_accuracy_per_class = {}\n",
    "        for class_label in range(1, 7):\n",
    "            binary_target = (df_filtered['target'] == class_label).astype(int)\n",
    "            binary_predicted = (df_filtered['predicted'] == class_label).astype(int)\n",
    "            user_accuracy = \"{:.4f}\".format(precision_score(binary_target, binary_predicted, zero_division=0))\n",
    "            producer_accuracy = \"{:.4f}\".format(recall_score(binary_target, binary_predicted, zero_division=0))\n",
    "            user_accuracy_per_class[f'User_Accuracy_class_{class_label}'] = user_accuracy\n",
    "            producer_accuracy_per_class[f'Producer_Accuracy_class_{class_label}'] = producer_accuracy\n",
    "\n",
    "        output_file.write(\"_\" * 70)\n",
    "        output_file.write(f\"\\nResult for {csv_file_name}:\\n\")\n",
    "        \n",
    "        output_file.write(\"\\nF1-scores for each class:\\n\")\n",
    "        for class_label, score in f1_scores.items():\n",
    "            output_file.write(f\"Class {class_label}: {score}\\n\") \n",
    "        \n",
    "        output_file.write(f\"\\nOverall Accuracy for all classes: {overall_accuracy_all_classes:.4f}\\n\")\n",
    "        output_file.write(f\"Overall Accuracy for class 6 (Forest): {overall_accuracy_class_6:.4f}\\n\")\n",
    "        output_file.write(f\"\\nCohen's Kappa for all classes: {kappa_score_all_classes:.4f}\\n\")\n",
    "        \n",
    "        auc_values = [float(value) for value in auc_per_class.values() if value != \"n/a\"]\n",
    "        auc_average = np.mean(auc_values) if auc_values else \"n/a\"\n",
    "        output_file.write(f\"\\nAUC average for all classes: {auc_average}\\n\")\n",
    "        \n",
    "        class_report = classification_report(\n",
    "            df_filtered['target'],\n",
    "            df_filtered['predicted'],\n",
    "            digits=4,\n",
    "        )\n",
    "        \n",
    "        output_file.write(f\"\\nClassification Report:\\n\")\n",
    "        output_file.write(class_report + '\\n')\n",
    "        \n",
    "        for class_label in range(1, 7):\n",
    "            output_file.write(f\"\\nUser Accuracy for class {class_label}: {user_accuracy_per_class[f'User_Accuracy_class_{class_label}']}\\n\")\n",
    "            output_file.write(f\"Producer Accuracy for class {class_label}: {producer_accuracy_per_class[f'Producer_Accuracy_class_{class_label}']}\\n\")\n",
    "\n",
    "        expected_labels = [1, 2, 3, 4, 5, 6]\n",
    "        conf_matrix = confusion_matrix(df_filtered['target'], df_filtered['predicted'], labels=expected_labels)\n",
    "\n",
    "        confusion_matrix_filename = os.path.splitext(csv_file_name)[0] + '_confusion_matrix.png'\n",
    "        confusion_matrix_filepath = os.path.join(confusion_matrix_folder, confusion_matrix_filename)\n",
    "        \n",
    "        labels = ['1', '2', '3', '4', '5', '6']\n",
    "        plt.figure(figsize=(10/2.54, 8/2.54))\n",
    "        ax = sns.heatmap(conf_matrix, annot=True, cmap=\"Greens\", fmt='.0f', cbar=True, xticklabels=expected_labels, yticklabels=expected_labels)\n",
    "\n",
    "        confusion_matrix_title = os.path.splitext(csv_file_name)[0][:-10]\n",
    "        ax.set_xlabel(\"Predicted\", fontsize=9)\n",
    "        ax.set_ylabel(\"Actual\", fontsize=9)\n",
    "        plt.title(f\"Confusion Matrix\", fontsize=9)\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), ha=\"right\", fontsize=9)\n",
    "        ax.set_yticklabels(ax.get_yticklabels(), fontsize=9)\n",
    "        for text in ax.texts:\n",
    "            text.set_fontsize(8)\n",
    "        plt.tight_layout(pad=1.5)\n",
    "        plt.savefig(confusion_matrix_filepath, bbox_inches='tight', pad_inches=0.1, dpi=800)\n",
    "        plt.close()\n",
    "        confusion_matrices.append(conf_matrix)\n",
    "\n",
    "        unclassified_points = len(df[~df['predicted'].between(1, 6)])\n",
    "\n",
    "print(\"Processing time in [s]\", time.process_time() - start)\n",
    "print(\"\\nAll results saved successfully :)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
